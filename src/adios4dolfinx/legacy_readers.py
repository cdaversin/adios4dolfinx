# Copyright (C) 2023 JÃ¸rgen Schartum Dokken
#
# This file is part of adios4dolfinx
#
# SPDX-License-Identifier:    MIT

import pathlib

import adios2
import basix
import dolfinx
import numpy as np
import ufl
from mpi4py import MPI

from .utils import compute_dofmap_pos, compute_local_range, index_owner
from .comm_helpers import send_cells_and_receive_dofmap_index, send_dofs_and_receive_values

__all__ = ["read_mesh_from_legacy_checkpoint", "read_mesh_from_legacy_h5",
           "read_function_from_legacy_h5"]


def read_mesh_from_legacy_h5(comm: MPI.Intracomm,
                             filename: pathlib.Path,
                             meshname: str) -> dolfinx.mesh.Mesh:
    """
    Read mesh from `h5`-file generated by legacy DOLFIN `HDF5File.write`.

    Args:
        comm: MPI communicator to distribute mesh over
        filename: Path to `h5` file (with extension)
        meshname: Name of mesh in `h5`-file
    """
    # Create ADIOS2 reader
    adios = adios2.ADIOS(comm)
    io = adios.DeclareIO("Mesh reader")
    io.SetEngine("HDF5")

    # Open ADIOS2 Reader
    infile = io.Open(str(filename), adios2.Mode.Read)
    # Get mesh topology (distributed)
    if f"{meshname}/topology" not in io.AvailableVariables().keys():
        raise KeyError(f"Mesh topology not found at '{meshname}/topology'")
    topology = io.InquireVariable(f"{meshname}/topology")
    shape = topology.Shape()
    local_range = compute_local_range(MPI.COMM_WORLD, shape[0])
    topology.SetSelection([[local_range[0], 0], [
                          local_range[1]-local_range[0], shape[1]]])

    mesh_topology = np.empty(
        (local_range[1]-local_range[0], shape[1]), dtype=np.int64)
    infile.Get(topology, mesh_topology, adios2.Mode.Sync)

    # Get mesh cell type
    if f"{meshname}/topology/celltype" not in io.AvailableAttributes().keys():
        raise KeyError(
            f"Mesh cell type not found at '{meshname}/topology/celltype'")
    celltype = io.InquireAttribute(f"{meshname}/topology/celltype")
    cell_type = celltype.DataString()[0]

    # Get mesh geometry
    if f"{meshname}/coordinates" not in io.AvailableVariables().keys():
        raise KeyError(
            f"Mesh coordintes not found at '{meshname}/coordinates'")
    geometry = io.InquireVariable(f"{meshname}/coordinates")
    shape = geometry.Shape()
    local_range = compute_local_range(MPI.COMM_WORLD, shape[0])
    geometry.SetSelection([[local_range[0], 0], [
                          local_range[1]-local_range[0], shape[1]]])
    mesh_geometry = np.empty(
        (local_range[1]-local_range[0], shape[1]), dtype=np.float64)
    infile.Get(geometry, mesh_geometry, adios2.Mode.Sync)

    assert adios.RemoveIO("Mesh reader")

    # Create DOLFINx mesh
    element = basix.ufl_wrapper.create_vector_element(
        basix.ElementFamily.P, cell_type, 1, basix.LagrangeVariant.equispaced,
        dim=mesh_geometry.shape[1], gdim=mesh_geometry.shape[1])
    domain = ufl.Mesh(element)
    return dolfinx.mesh.create_mesh(MPI.COMM_WORLD, mesh_topology, mesh_geometry, domain)


def read_mesh_from_legacy_checkpoint(
        filename: str, cell_type: str = "tetrahedron") -> dolfinx.mesh.Mesh:
    """
    Read mesh from `h5`-file generated by legacy DOLFIN `XDMFFile.write_checkpoint`.
    Needs to get the `cell_type` as input, as legacy DOLFIN does not store the cell-type in the
    `h5`-file

    Args:
        filename: Path to `h5` file (with extension)
        celltype: String describing cell-type
    """

    adios = adios2.ADIOS(MPI.COMM_WORLD)
    io = adios.DeclareIO("Mesh reader")
    io.SetEngine("HDF5")

    # Open ADIOS2 Reader
    infile = io.Open(filename, adios2.Mode.Read)
    path = "func/func_0"
    # Get mesh topology (distributed)
    if f"{path}/topology" not in io.AvailableVariables().keys():
        raise KeyError(f"Mesh topology not found at '{path}topology'")
    topology = io.InquireVariable(f"{path}/topology")
    shape = topology.Shape()
    local_range = compute_local_range(MPI.COMM_WORLD, shape[0])
    topology.SetSelection([[local_range[0], 0], [
                          local_range[1]-local_range[0], shape[1]]])

    mesh_topology = np.empty(
        (local_range[1]-local_range[0], shape[1]), dtype=np.int32)
    infile.Get(topology, mesh_topology, adios2.Mode.Sync)

    # Get mesh geometry
    if f"{path}/geometry" not in io.AvailableVariables().keys():
        raise KeyError(
            f"Mesh geometry not found at '{path}/geometry'")
    geometry = io.InquireVariable(f"{path}/geometry")
    shape = geometry.Shape()
    local_range = compute_local_range(MPI.COMM_WORLD, shape[0])
    geometry.SetSelection([[local_range[0], 0], [
                          local_range[1]-local_range[0], shape[1]]])
    mesh_geometry = np.empty(
        (local_range[1]-local_range[0], shape[1]), dtype=np.float64)
    infile.Get(geometry, mesh_geometry, adios2.Mode.Sync)

    assert adios.RemoveIO("Mesh reader")

    # Create DOLFINx mesh
    element = basix.ufl_wrapper.create_vector_element(
        basix.ElementFamily.P, cell_type, 1, basix.LagrangeVariant.equispaced,
        dim=mesh_geometry.shape[1], gdim=mesh_geometry.shape[1])
    domain = ufl.Mesh(element)

    return dolfinx.mesh.create_mesh(
        MPI.COMM_WORLD, mesh_topology, mesh_geometry, domain)


def read_function_from_legacy_h5(comm: MPI.Intracomm, filename: pathlib.Path,
                                 u: dolfinx.fem.Function):
    V = u.function_space
    mesh = u.function_space.mesh

    # ----------------------Step 1---------------------------------
    # Compute index of input cells, and position in input dofmap
    local_cells, dof_pos = compute_dofmap_pos(u.function_space)
    input_cells = mesh.topology.original_cell_index[local_cells]

    # Compute mesh->input communicator
    # 1.1 Compute mesh->input communicator
    num_cells_global = mesh.topology.index_map(mesh.topology.dim).size_global
    owners = index_owner(mesh.comm, input_cells, num_cells_global)
    unique_owners = np.unique(owners)
    # FIXME: In C++ use NBX to find neighbourhood
    _tmp_comm = mesh.comm.Create_dist_graph(
        [mesh.comm.rank], [len(unique_owners)], unique_owners, reorder=False)
    source, dest, _ = _tmp_comm.Get_dist_neighbors()

    # ----------------------Step 2---------------------------------
    # Get global dofmap indices from input process
    num_cells_global = mesh.topology.index_map(mesh.topology.dim).size_global
    dofmap_indices = send_cells_and_receive_dofmap_index(
        filename, comm, np.asarray(source, dtype=np.int32), np.asarray(
            dest, dtype=np.int32), owners, input_cells, dof_pos,
        num_cells_global, "/mesh/cell_dofs", "/mesh/x_cell_dofs", "HDF5")

    # ----------------------Step 3---------------------------------
    # Compute owner of global dof on distributed mesh
    num_dof_global = V.dofmap.index_map_bs * V.dofmap.index_map.size_global
    dof_owner = index_owner(mesh.comm, dofmap_indices, num_dof_global)
    # Create MPI neigh comm to owner.
    # NOTE: USE NBX in C++
    unique_dof_owners = np.unique(dof_owner)
    mesh_to_dof_comm = mesh.comm.Create_dist_graph(
        [mesh.comm.rank], [len(unique_dof_owners)], unique_dof_owners, reorder=False)
    dof_source, dof_dest, _ = mesh_to_dof_comm.Get_dist_neighbors()

    # Send global dof indices to correct input process, and recieve value of given dof
    local_values = send_dofs_and_receive_values(filename, "/mesh/vector_0", "HDF5",
                                                comm,   np.asarray(dof_source, dtype=np.int32),
                                                np.asarray(dof_dest, dtype=np.int32), dofmap_indices, dof_owner)
    # ----------------------Step 4---------------------------------
    # Populate local part of array and scatter forward
    u.x.array[:len(local_values)] = local_values
    u.x.scatter_forward()
